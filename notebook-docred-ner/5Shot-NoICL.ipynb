{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a77d199-8de5-4b50-8b07-bbc5f731cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2481a527-e4e2-49a6-91e4-f94b4993b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from auto_chatgpt.autochatgpt.chatgptbot import ChatGPTBot\n",
    "from ner import NER\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44b025f-90b4-4bd2-b5cb-45ffe587e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kindly update based on your email and password. \n",
    "# For every experiment, change the file_name. This will the file to store the outcome of ChatGPT. \n",
    "# This is to esnure, we do not lose any output if incase there is any unexpected disruption.\n",
    "EMAIL_ADDRESS = \"..\"\n",
    "PASSWORD = \"..\"\n",
    "FILE_NAME = \"5Shot-NoICL\"\n",
    "FEW_SHOT=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29ff55a-e5e6-47b5-a508-fcb6817e4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ChatGPT Bot\n",
    "new_chat = ChatGPTBot(EMAIL_ADDRESS, PASSWORD, headless=False, wait=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32dcaf-d5b3-4671-bcc3-c8ac8cd81fab",
   "metadata": {},
   "source": [
    "## Note:\n",
    "1. Do not remove headless variable. This will ensure us to monitor input and output from ChatGPT. Also it will be easier to indentify if incase there is any issue happened during scrapping.\n",
    "2. Once the above cell complete run, you will see a new browser opens and shows like this:\n",
    "   1. <img src=\"../images/chatgpt-popup.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "3. <code style=\"background:yellow;color:black\">Close the popup first before move to run next Jupyter Cell.</code> \n",
    "4. <code style=\"background:yellow;color:black\">Now Select New Chat before running every Experiment.</code>\n",
    "5. Then you can start running below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0425067-62e3-4eb5-bab6-220e98683c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = NER(FEWSHOT=FEW_SHOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68eb28e-65bd-4037-afbc-3d64b455b483",
   "metadata": {},
   "source": [
    "## Few Shot Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85dd3906-c824-4856-9fb3-adfe8de2dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../informative_samples/docred-experiment/docred.json\",\"r\") as f:\n",
    "    few_shot = json.load(f)\n",
    "few_shot = few_shot[:FEW_SHOT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "852e896b-1fa7-49cc-abcb-abc921b7c196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Article\n",
      "Writing Article\n",
      "Writing Article\n",
      "Writing Article\n",
      "Writing Article\n"
     ]
    }
   ],
   "source": [
    "new_chat.send_prompt(prompt=\"Now you will be provided with some sample of News Articles and the Entity Extracted from Them. This Entity are extracted based on the Annotation Guideline and Context of News Article\")\n",
    "for article in few_shot[:FEW_SHOT]:\n",
    "    print(\"Writing Article\")\n",
    "    article_tr = \" \".join([\" \".join(sentence) for sentence in article[\"sents\"]])\n",
    "    article[\"vertexSet\"] = [item for sublist in article[\"vertexSet\"] for item in sublist]\n",
    "    entities = [f'{indx}. {article[\"vertexSet\"][indx][\"name\"]} and entity label is {article[\"vertexSet\"][indx][\"type\"]}' for indx in range(len(article[\"vertexSet\"])) if article[\"vertexSet\"][indx][\"type\"] in [\"PER\",\"LOC\",\"ORG\"]]\n",
    "    prompt = \"Here is the news article {}: Here is the extracted entities{}\".format(article_tr,entities)\n",
    "    new_chat.send_prompt(prompt=prompt)\n",
    "    time.sleep(random.randint(15,20))\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf88e6-6531-48c2-af65-70f5f5f60390",
   "metadata": {},
   "source": [
    "## Start Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed0b54f4-309f-48d7-b9df-7944e1e5b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset. \n",
    "dt = \"../dataset/docred-dataset.json\"\n",
    "with open(dt) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "dataset = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4efc49-68b8-46e4-83aa-e5bb3171c64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Milestone Completion: 0\n",
      "You will annotate: 200 News Articles\n"
     ]
    }
   ],
   "source": [
    "# Define the file to save ChatGPT output. \n",
    "# If the file is existing (if you are running after first time), then it will automatically find the articles that are annotated\n",
    "file = f\"ner_annotation_by_chatgpt/{FILE_NAME}.json\"\n",
    "counter_time = 1\n",
    "\n",
    "if os.path.exists(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        existing_data = json.load(f)\n",
    "else:\n",
    "    existing_data = []\n",
    "\n",
    "total = dataset.shape[0]\n",
    "len_existing_data = len(existing_data)\n",
    "print(\"Current Milestone Completion: {}\".format(len_existing_data))\n",
    "print(\"You will annotate: {} News Articles\".format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed6fbda2-cc56-49d7-958c-736461953818",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in dataset.iloc[len_existing_data:total].iterrows():\n",
    "    print(\"Annotating News Article.... {}\".format(idx+1))\n",
    "    article_tr = \" \".join([\" \".join(sentence) for sentence in row[\"sents\"]])\n",
    "    prompt = \"Act as Data Annotator, identify and extract all the entity PERSON, LOCATION, ORGANIZATION, from the input news article. Input: {}. Provide the output as JSON.\".format(article_tr)\n",
    "    new_chat.send_prompt(prompt=prompt)\n",
    "\n",
    "    # This counter is to set and idle time and prvent from spamming ChatGPT website\n",
    "    if counter_time==25:\n",
    "        time.sleep(random.randint(45,60))\n",
    "        counter_time=1\n",
    "    elif counter_time==15:\n",
    "        time.sleep(random.randint(20,30))\n",
    "        counter_time=counter_time+1\n",
    "    else:\n",
    "        time.sleep(random.randint(8,10))\n",
    "        counter_time=counter_time+1\n",
    "\n",
    "    # Get ChatGPT output for the input given\n",
    "    res = new_chat.get_gpt_response()\n",
    "    time.sleep(random.randint(5,7))\n",
    "    response = res[-1]\n",
    "    time.sleep(random.randint(2,4))\n",
    "    # Format ChatGPT outcome and extract only required output.\n",
    "    formated_response = ner.formatting_chatgpt_response_docred(response)\n",
    "    # Get the offset from each entity.\n",
    "    validated_entity_set = ner.find_offset(article_tr,formated_response) #Only Have Exact Match\n",
    "\n",
    "    # Save the Gold Annotation and Format them similar to existing format.\n",
    "    ent_labels = {\"PER\":[],\"LOC\":[],\"ORG\":[]}\n",
    "    flat_list = [item for sublist in row[\"vertexSet\"] for item in sublist]\n",
    "    for ent in flat_list:\n",
    "        if ent['type'] in ent_labels:\n",
    "            ent_labels[ent['type']].append(ent['name'])\n",
    "    gold = ner.find_offset(article_tr,ent_labels) #Only Have Exact Match\n",
    "\n",
    "    existing_data.append({\n",
    "        \"idx\": idx,\n",
    "        \"text\": article_tr,\n",
    "        \"chatgpt_ent\": validated_entity_set,\n",
    "        \"gold_ent\": gold\n",
    "    })\n",
    "    with open(file, \"w\") as f:\n",
    "        json.dump(existing_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644ef3b-a08a-4a55-a633-fc5aa7683f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgpt",
   "language": "python",
   "name": "cgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
