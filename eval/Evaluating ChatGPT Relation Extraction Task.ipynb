{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb9ad967-d570-4b53-82f9-6c6b15b6d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "14979a63-fc27-4a39-bb95-fcd3577e761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file = \"ZeroShot-Guideline\"\n",
    "# Choose either DocRED Label or ACE05 Label\n",
    "label_dataset = \"ace05\" # docred\n",
    "# Choose dataset either MEN-Dataset, DocRED-Dataset\n",
    "dataset = \"men\" # docred \n",
    "folder = f\"notebook-{dataset}-rel-{label_dataset}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6cfde0af-192a-4761-b9c8-b998b2a41178",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHATGPT = f\"../{folder}/rel_annotation_by_chatgpt/{main_file}.json\"\n",
    "with open(OUTPUT_CHATGPT,\"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "00a8ce1b-27d9-4f4e-baab-43d79f242aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if label_dataset == \"ace05\":\n",
    "    list_labels = pd.read_csv(\"./guideline/Relation Labels - ACE05.csv\")[\"Relation Label\"].to_list()\n",
    "if label_dataset == \"docred\":\n",
    "    list_labels = pd.read_csv(\"./guideline/Relation Labels - DocRED.csv\")[\"Relation Label\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9c045c97-1e07-4d09-b973-d34369b9f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store predicted and actual relations\n",
    "precision_recall_f1 = []\n",
    "\n",
    "# Extract predicted and actual relations from the sample data\n",
    "for idx in range(len(data)):\n",
    "    chatgpt_ordered = []\n",
    "    gold_ordered = []\n",
    "    if data[idx][\"chatgpt\"] == \"Failed to extract relation from JSON\":\n",
    "        data[idx][\"chatgpt\"] = []\n",
    "  \n",
    "    gold = [rel for rel in data[idx][\"gold\"] if rel[\"relation\"] in list_labels]\n",
    "    chatgpt = [rel for rel in data[idx][\"chatgpt\"] if rel[\"relation\"] in list_labels]\n",
    "    \n",
    "    # Create sets of predicted and true relations\n",
    "    predicted_relations_set = set(tuple(sorted(rel['entity_pair'].values())) for rel in chatgpt)\n",
    "    true_relations_set = set(tuple(sorted(rel['entity_pair'].values())) for rel in gold)\n",
    "    \n",
    "    # Calculate TP, FP, FN\n",
    "    TP = len(predicted_relations_set.intersection(true_relations_set))\n",
    "    FP = len(predicted_relations_set.difference(true_relations_set))\n",
    "    FN = len(true_relations_set.difference(predicted_relations_set))\n",
    "           \n",
    "    # Calculate precision, recall, and F1-Score\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    precision_recall_f1.append({\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1_score\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "cec0e6c0-448a-45b2-8120-5421e474638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Precision: 0.264\n",
      "Weighted Average Recall: 0.665\n",
      "Weighted Average F1-Score: 0.356\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to keep track of totals\n",
    "total_precision = 0.0\n",
    "total_recall = 0.0\n",
    "total_f1 = 0.0\n",
    "\n",
    "# Loop through each dictionary\n",
    "for entry in precision_recall_f1:\n",
    "    total_precision += entry['Precision']\n",
    "    total_recall += entry['Recall']\n",
    "    total_f1 += entry['F1-Score']\n",
    "\n",
    "# Calculate the weighted average F1-score\n",
    "total_entries = len(precision_recall_f1)\n",
    "weighted_average_f1 = total_f1 / total_entries\n",
    "\n",
    "print(\"Weighted Average Precision:\", round(total_precision / total_entries,3))\n",
    "print(\"Weighted Average Recall:\", round(total_recall / total_entries,3))\n",
    "print(\"Weighted Average F1-Score:\", round(weighted_average_f1,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt-eval-win",
   "language": "python",
   "name": "chatgpt-eval-win"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
